# Neural Networks: How can you trust deep models that seem like black boxes?

**2024.03.26, 2024.04.02, 2024.04.09**

## Lecture outline

Neural Network (NN): it's biological origin and how it relates to data science

### Perceptrons

The basic structure of a neural network!

**Activation function**:
- Heaviside step function
- logistic function (or $tanh$)
- Rectified linear unit (ReLU)
- Softplus

Limitation on non-linear separation

### Multi-layer Perceptrons (MLP)

The simplest NN that has practical use!

Hidden layer: transform the linear perceptron model into a non-linear model.

Predictors, parameters, and loss function of MLP
- number of observations vs parameters

Challenges of a non-linear optimization
- curse of dimensionality? 
- potential solution: Ensembles

What's the advantage?

MLP: a Universal approximator!

### Extreme learning Machine (ELM)

The idea of using ensembles -> transform a non-linear problem back to a linear problem

What's the advantage?

How to determine the number of hidden layer nodes $L$?

Skip connection

### RBF

TBD

## Slides (Just for my convenience; will be removed after today's class due to copyright issues)

[Slides](https://docs.google.com/presentation/d/17eIMIMbfPACRaUueoTIXn2FfH2eFSqOPnURORtjEc3g/edit?usp=sharing)
